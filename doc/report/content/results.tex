\subsection{Network Simulations}

%\subsubsection{Spatial Pooler}

The spatial pooler properties were investigated for a network spanning 1,000 columns and an input vector of size 10,000. To speed up the simulation, the input vector was multiplied to the feed forward connectivity matrix yielding a vector $\vec{l}$ containing the number of active inputs per column. This allowed to simulate the model with only 1,000 spike sources. Each source was configured to emit $l_i$ normally distributed events within a very small time window, simulating the same number of coincident events from multiple input vector elements.

A first experiment was designed to verify the basic \gls{kwta} functionality. A random pattern was presented to the network. The number of active inputs per column -- the input overlap score -- can be visualized in a histogram as shown in figure~\ref{fig:spatial_pooler_activity}. By highlighting the active columns, one can investigate the network's selection criteria. Complying with the requirements for a spatial pooler, only the rightmost bars -- representing columns with the highest input counts -- are highlighted. Furthermore, the model's capability to resolve ties between columns receiving the same input counts is demonstrated: the bar at the decision boundary was not selected as a whole but only a few columns were picked. This verifies spatial pooler property~\ref{enm:spatial_pooler_selection}.

In a second scenario, input vectors with varying sparsity were fed into the network, as shown in figure~\ref{fig:spatial_pooler_sparsity}. The number of active columns stays constant across a wide range of input sparsity. Additionally the plot shows that columns must receive a minimum amount of input to become active at all. This verifies the underlaying \gls{kwta} approach as well as spatial pooler properties~\ref{enm:spatial_pooler_sparsity} and~\ref{enm:spatial_pooler_minimum}.

To verify the general functionality of a spatial pooler, expressed in property~\ref{enm:spatial_pooler_overlap}, a third experiment was set up. We generated input data sets with a variable overlap starting from an initial random binary vector. For each stimulus, the overlap of the columnar activity with the initial dataset was calculated while sweeping the input's overlap. The resulting relation of input and output overlap scores is shown in figure~\ref{fig:spatial_pooler_overlap}. Also included are the results of a similar experiment performed with a custom Python implementation of the spatial pooler directly following the original specification \citep{numenta2011htm}. In general, the curve yielded by the \gls{lif}-based implementation matches well with the algorithmic version of the model. However, maximal and minimal overlap scores are not reached for extreme input values $0.0$ and $1.0$.

\begin{figure*}
	\begin{center}
		\input{assets/spatial_pooler/histo/activity.pgf}
	\end{center}
	\caption{Histogram showing the distribution of overlap scores individual columns see. Active columns are highlighted in this plot, showing that the $k$ columns with the most input were successfully selected.} 
	\label{fig:spatial_pooler_activity}
\end{figure*}

\begin{figure*}
	\begin{center}
		\input{assets/spatial_pooler/sparsity/sparsity.pgf}
	\end{center}
	\caption{The columnar sparsity is shown as a dependency of the input vector's sparsity. The highlighted range contains data points deviating not more than \SI{10}{\%} from the target sparsity of \SI{4}{\%}. Error bars indicate the standard deviation across five trials. The number of active columns stays constant for a broad range of input sparsity, thus fulfilling one of the basic requirements of the spatial pooler.} 
	\label{fig:spatial_pooler_sparsity}
\end{figure*}

\begin{figure*}
	\begin{center}
		\input{assets/spatial_pooler/overlap/overlap.pgf}
	\end{center}
	\caption{Output overlap as a dependency of the input vector's overlap. Similar input gets mapped to similar output patterns, while disjunct input results in low overlap scores. Results from a software-based implementation is included as a reference in this plot.}
	\label{fig:spatial_pooler_overlap}
\end{figure*}

%\subsubsection{Learning}

Realizing online learning mechanisms in neuromorphic hardware represents a huge challenge. In order to follow the \gls{htm} specification as close possible, a supervised update rule was implemented: for each time step, a matrix containing permanence values for every connection is updated according to the activity patterns of the previous time step. This allows to implement the concepts of structural plasticity presented in the original white paper. Exeriments to replace the used form of structural plasticity by a classic nearest-neighbor \gls{stdp} model did not yield the desired results.

\begin{figure*}
	\begin{center}
		\input{assets/spatial_pooler/learning/learning.pgf}
	\end{center}
	\caption{Dependency of output and input overlap for a trained spatial pooler. Results of five independent simulation runs are shown as well as reference data from a custom software implementation.}
	\label{fig:spatial_pooler_learning}
\end{figure*}

Simulation results of the chosen implementation are shown in figure~\ref{fig:spatial_pooler_learning}. For the target platform, the learning algorithms could be implemented on the \gls{ppu} which is planned for the next version of the \gls{hicann} chip \citep{friedmann2013phd}.

The experiments have shown that the model presented in this section does fulfill the requirements for a spatial pooler and can be considered a solid \gls{kwta} implementation. The specific results of course depend on the individual network's size and configuration. In this case, the network -- most importantly the columnar neurons' time constants -- was configured for a relatively short time step of $T = \SI{50}{\milli\second}$. By choosing different parameter sets, the network can be tuned towards different operational scenarios, e.g. further increasing the model's stability.

%\subsubsection{Temporal Memory}

Lorem ipsum dolor sit amet.

\begin{figure*}
	\begin{center}
		\input{assets/temporal_memory/live.pgf}
	\end{center}
	\caption{A \gls{lif} neuron based temporal memory implementation correctly predicting different patterns. Predicted cells are marked blue, active cells in purple. The network spans 128 columns with each of their eight \gls{htm} cells collecting distal stimuli via two dendritic segments. Connectivity for the distal inputs was configured externally. The model was presented three disjunct sequences of size three. The individual patterns were separated by a random input \gls{sdr}.}
	\label{fig:static_temporal_memory_live}
\end{figure*}

\subsection{Learning Algorithms}

\subsection{Place and Route}
